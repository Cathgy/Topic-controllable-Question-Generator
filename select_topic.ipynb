{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def find_best_matching_title(question_titles, text_annotations):\n",
    "    matching_titles = []\n",
    "    for question_title in question_titles:\n",
    "        for annotation_group in text_annotations:\n",
    "            for annotation in annotation_group['annotation_data']:\n",
    "                if question_title['title'] == annotation['title']:\n",
    "                    matching_titles.append((annotation['title'], annotation['pageRank']))\n",
    "\n",
    "    if not matching_titles:\n",
    "        return \"NA\"\n",
    "    \n",
    "    best_title = max(matching_titles, key=lambda x: x[1])\n",
    "    return best_title[0]\n",
    "\n",
    "\n",
    "def process_data(question_data, text_data):\n",
    "    enriched_data = []\n",
    "\n",
    "    for question_entry in question_data:\n",
    "        question_id = question_entry['id']\n",
    "        question_text = question_entry['text']\n",
    "        \n",
    "        \n",
    "        if 'annotations' in question_entry and 'annotation_data' in question_entry['annotations']:\n",
    "            question_titles = question_entry['annotations']['annotation_data']\n",
    "        else:\n",
    "            question_titles = []\n",
    "        \n",
    "        \n",
    "        matching_text_entry = next((entry for entry in text_data if entry['id'] == question_id), None)\n",
    "\n",
    "        if matching_text_entry:\n",
    "            text_content = matching_text_entry['text']\n",
    "            text_annotations = matching_text_entry['annotations']\n",
    "\n",
    "            \n",
    "            topic = find_best_matching_title(question_titles, text_annotations)\n",
    "            \n",
    "            \n",
    "            enriched_entry = {\n",
    "                \"id\": question_id,\n",
    "                \"question\": question_text,\n",
    "                \"text\": text_content,\n",
    "                \"topic\": topic\n",
    "            }\n",
    "            enriched_data.append(enriched_entry)\n",
    "\n",
    "    return enriched_data\n",
    "\n",
    "\n",
    "def main():\n",
    "  \n",
    "    question_data = load_json_data('/home/qiyu/Dev/ziqing/wiki/wikified_eval_squad_question.json')\n",
    "    text_data = load_json_data('/home/qiyu/Dev/ziqing/wiki/wikified_eval_squad_text.json')\n",
    "\n",
    "\n",
    "    enriched_data = process_data(question_data, text_data)\n",
    "\n",
    "\n",
    "    with open('enriched_eval_squad.json', 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(enriched_data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_entries_in_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        num_entries = len(data)\n",
    "    \n",
    "\n",
    "    print(f\"'{file_path}':{num_entries}\")\n",
    "\n",
    "file_path = '/home/qiyu/Dev/ziqing/wiki/enriched_eval_squad.json'\n",
    "\n",
    "count_entries_in_json(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na_topics(file_path, output_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "\n",
    "    filtered_data = [entry for entry in data if entry['topic'] != \"NA\"]\n",
    "    \n",
    "\n",
    "    num_filtered_entries = len(filtered_data)\n",
    "    print(f\"after: {num_filtered_entries}\")\n",
    "    \n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(filtered_data, output_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"write in '{output_path}'\")\n",
    "\n",
    "\n",
    "file_path = '/home/qiyu/Dev/ziqing/wiki/enriched_eval_squad.json'\n",
    "\n",
    "output_path = '/home/qiyu/Dev/ziqing/wiki/filtered_enriched_eval_squad.json'\n",
    "\n",
    "remove_na_topics(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def count_id_groups(data):\n",
    "   \n",
    "    id_groups = defaultdict(list)\n",
    "    \n",
    "    for entry in data:\n",
    "        entry_id = entry['id']\n",
    "        id_groups[entry_id].append(entry)\n",
    "    \n",
    "    num_groups = len(id_groups)\n",
    "    \n",
    "    return num_groups\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_path = 'filtered_enriched_khan_computing.json'\n",
    "    data = load_json_data(file_path)\n",
    "    num_groups = count_id_groups(data)\n",
    "    print(f\" '{file_path}' : {num_groups}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/qiyu/Dev/ziqing/wiki/wikified_eval_squad_question.json', 'r', encoding='utf-8') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "with open('/home/qiyu/Dev/ziqing/wiki/wikified_eval_squad_text.json', 'r', encoding='utf-8') as f:\n",
    "    text_data = json.load(f)\n",
    "\n",
    "\n",
    "text_data_index = {}\n",
    "\n",
    "for item in text_data:\n",
    "    ids = item['id'].split(', ')\n",
    "    for id_ in ids:\n",
    "        text_data_index[id_] = item\n",
    "\n",
    "\n",
    "enriched_squad = []\n",
    "\n",
    "\n",
    "for question in questions_data:\n",
    "    question_id = question.get('id')\n",
    "    question_text = question.get('text')\n",
    "    question_annotations = question.get('annotations', {})\n",
    "\n",
    "    if not question_annotations:\n",
    "        continue\n",
    "\n",
    "    question_annotation_data = question_annotations.get('annotation_data', [])\n",
    "\n",
    "    if not question_annotation_data:\n",
    "        continue\n",
    "\n",
    "\n",
    "    text_item = text_data_index.get(question_id)\n",
    "\n",
    "    if not text_item:\n",
    "        continue\n",
    "\n",
    "    text_text = text_item.get('text')\n",
    "    text_annotations = text_item.get('annotations', [])\n",
    "\n",
    "    if not text_annotations:\n",
    "        continue\n",
    "\n",
    "\n",
    "    found_titles = []\n",
    "    for annotation in question_annotation_data:\n",
    "        q_title = annotation.get('title')\n",
    "        q_pageRank = annotation.get('pageRank', 0)\n",
    "\n",
    "        for text_annotation in text_annotations:\n",
    "            for t_annotation in text_annotation.get('annotation_data', []):\n",
    "                t_title = t_annotation.get('title')\n",
    "\n",
    "                if q_title == t_title:\n",
    "                    found_titles.append((q_title, q_pageRank))\n",
    "\n",
    "\n",
    "    if not found_titles:\n",
    "        selected_title = \"NA\"\n",
    "    elif len(found_titles) == 1:\n",
    "        selected_title = found_titles[0][0]\n",
    "    else:\n",
    "      \n",
    "        selected_title = max(found_titles, key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "    enriched_entry = {\n",
    "        \"id\": question_id,\n",
    "        \"question\": question_text,\n",
    "        \"text\": text_text,\n",
    "        \"topic\": selected_title\n",
    "    }\n",
    "\n",
    "    enriched_squad.append(enriched_entry)\n",
    "\n",
    "\n",
    "with open('new_enriched_squad.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(enriched_squad, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Enriched squad data saved to enriched_squad.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_entries_in_json(file_path):\n",
    "   \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "\n",
    "    num_entries = len(data)\n",
    "    \n",
    "\n",
    "    print(f\" '{file_path}' : {num_entries}\")\n",
    "\n",
    "\n",
    "file_path = '/home/qiyu/Dev/ziqing/wiki/new_enriched_squad.json'\n",
    "\n",
    "count_entries_in_json(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na_topics(file_path, output_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    filtered_data = [entry for entry in data if entry['topic'] != \"NA\"]\n",
    "\n",
    "    num_filtered_entries = len(filtered_data)\n",
    "    print(f\"after: {num_filtered_entries}\")\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(filtered_data, output_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"write '{output_path}'\")\n",
    "\n",
    "file_path = '/home/qiyu/Dev/ziqing/wiki/enriched_eval_squad.json'\n",
    "\n",
    "output_path = 'filtered_eval_enriched_squad.json'\n",
    "\n",
    "remove_na_topics(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def find_highest_page_rank(titles, annotations):\n",
    "    max_page_rank = -1\n",
    "    selected_title = \"NA\"\n",
    "    for annotation in annotations:\n",
    "        if annotation['title'] in titles and annotation['pageRank'] > max_page_rank:\n",
    "            max_page_rank = annotation['pageRank']\n",
    "            selected_title = annotation['title']\n",
    "    return selected_title\n",
    "\n",
    "src_data = load_data('/home/qiyu/Dev/ziqing/wiki/wikified_src_test1.json')\n",
    "tgt_data = load_data('/home/qiyu/Dev/ziqing/wiki/wikified_tgt_test1.json')\n",
    "\n",
    "src_data = src_data if isinstance(src_data, list) else [src_data]\n",
    "tgt_data = tgt_data if isinstance(tgt_data, list) else [tgt_data]\n",
    "\n",
    "enriched_data = []\n",
    "\n",
    "for src_item in src_data:\n",
    "    for tgt_item in tgt_data:\n",
    "        if src_item['id'] == tgt_item['id']:\n",
    "            src_titles = {annot['title'] for annot in src_item['annotations']['annotation_data']}\n",
    "            tgt_titles = {annot['title']: annot for annot in tgt_item['annotations']['annotation_data']}\n",
    "            \n",
    "            common_titles = src_titles.intersection(tgt_titles.keys())\n",
    "            \n",
    "            if not common_titles:\n",
    "                topic = \"NA\"\n",
    "            elif len(common_titles) == 1:\n",
    "                topic = common_titles.pop()\n",
    "            else:\n",
    "                topic = find_highest_page_rank(common_titles, tgt_item['annotations']['annotation_data'])\n",
    "            \n",
    "            enriched_data.append({\n",
    "                'id': src_item['id'],\n",
    "                'sentence': src_item['text'],\n",
    "                'question': tgt_item['text'],\n",
    "                'topic': topic\n",
    "            })\n",
    "\n",
    "output_path = Path('/home/qiyu/Dev/ziqing/wiki/enriched_khanexperiment1.json')\n",
    "with output_path.open('w', encoding='utf-8') as file:\n",
    "    json.dump(enriched_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"save as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json_data(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "enriched_data = load_json_data('/home/qiyu/Dev/ziqing/wiki/enriched_khanexperiment1.json')\n",
    "para_data = load_json_data('/home/qiyu/Dev/ziqing/wiki/para_test.json')\n",
    "\n",
    "para_dict = {item['id']: item['text'] for item in para_data}\n",
    "\n",
    "filtered_enriched_data = []\n",
    "for item in enriched_data:\n",
    "    if item['topic'] != \"NA\" and item['id'] in para_dict:\n",
    "        new_item = {\n",
    "            'text': para_dict[item['id']],\n",
    "            'sentence': item['sentence'],\n",
    "            'question': item['question'],\n",
    "            'topic': item['topic']\n",
    "        }\n",
    "        filtered_enriched_data.append(new_item)\n",
    "\n",
    "save_json_data(filtered_enriched_data, '/home/qiyu/Dev/ziqing/wiki/filtered_enriched_khanexperiment1.json')\n",
    "\n",
    "print(\"save as 'filtered_enriched_khanexperiment1.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def save_json_data(data, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "def trim_text_to_context(text, sentence):\n",
    "    start_index = text.find(sentence)\n",
    "    if start_index == -1:\n",
    "        return sentence  \n",
    "        \n",
    "    pre_start = max(0, start_index - 500) \n",
    "    \n",
    "    period_before = text.rfind('.', 0, pre_start)\n",
    "    start_pos = period_before + 1 if period_before != -1 else pre_start\n",
    "\n",
    "    post_end = min(len(text), start_index + len(sentence) + 500) \n",
    "    period_after = text.find('.', start_index + len(sentence), post_end)\n",
    "    end_pos = period_after + 1 if period_after != -1 else post_end\n",
    "\n",
    "    if end_pos - start_pos > 1200:\n",
    "        excess = (end_pos - start_pos) - 1200\n",
    "        start_pos += excess // 2\n",
    "        end_pos -= excess - (excess // 2)\n",
    "\n",
    "    return text[start_pos:end_pos].strip()\n",
    "\n",
    "data = load_json_data('/home/qiyu/Dev/ziqing/wiki/filtered_enriched_khanexperiment1.json')\n",
    "\n",
    "updated_data = []\n",
    "for item in data:\n",
    "    trimmed_text = trim_text_to_context(item['text'], item['sentence'])\n",
    "    updated_data.append({\n",
    "        'text': trimmed_text,\n",
    "        'sentence': item['sentence'],\n",
    "        'question': item['question'],\n",
    "        'topic': item['topic']\n",
    "    })\n",
    "\n",
    "save_json_data(updated_data, '/home/qiyu/Dev/ziqing/wiki/readyfiltered_khanexperiment1.json')\n",
    "\n",
    "print(\"save as 'readyfiltered_khanexperiment1.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_texts(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    new_data = [{\"id\": idx, \"text\": item[\"text\"]} for idx, item in enumerate(data)]\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(new_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "extract_texts('readyfiltered_khanexperiment1.json', 'khanexperiment1_text.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def find_highest_page_rank(titles, annotations):\n",
    "    max_page_rank = -1\n",
    "    selected_title = \"NA\"\n",
    "    for annotation in annotations:\n",
    "        if annotation['title'] in titles and annotation['pageRank'] > max_page_rank:\n",
    "            max_page_rank = annotation['pageRank']\n",
    "            selected_title = annotation['title']\n",
    "    return selected_title\n",
    "\n",
    "src_data = load_data('/home/qiyu/Dev/ziqing/wiki/wikified_KhanQ_text.json')\n",
    "tgt_data = load_data('/home/qiyu/Dev/ziqing/wiki/wikified_KhanQ_question.json')\n",
    "\n",
    "src_data = src_data if isinstance(src_data, list) else [src_data]\n",
    "tgt_data = tgt_data if isinstance(tgt_data, list) else [tgt_data]\n",
    "\n",
    "enriched_data = []\n",
    "\n",
    "for src_item in src_data:\n",
    "    for tgt_item in tgt_data:\n",
    "        if src_item['id'] == tgt_item['id']:\n",
    "            src_titles = {annot['title'] for annot in src_item['annotations']['annotation_data']}\n",
    "            tgt_titles = {annot['title']: annot for annot in tgt_item['annotations']['annotation_data']}\n",
    "            \n",
    "            common_titles = src_titles.intersection(tgt_titles.keys())\n",
    "            \n",
    "            if not common_titles:\n",
    "                topic = \"NA\"\n",
    "            elif len(common_titles) == 1:\n",
    "                topic = common_titles.pop()\n",
    "            else:\n",
    "                topic = find_highest_page_rank(common_titles, tgt_item['annotations']['annotation_data'])\n",
    "            \n",
    "            enriched_data.append({\n",
    "                'id': src_item['id'],\n",
    "                'text': src_item['text'],\n",
    "                'question': tgt_item['text'],\n",
    "                'topic': topic\n",
    "            })\n",
    "\n",
    "output_path = Path('/home/qiyu/Dev/ziqing/wiki/enriched_KhanQ.json')\n",
    "with output_path.open('w', encoding='utf-8') as file:\n",
    "    json.dump(enriched_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"save as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na_topics(file_path, output_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    filtered_data = [entry for entry in data if entry['topic'] != \"NA\"]\n",
    "    \n",
    "    num_filtered_entries = len(filtered_data)\n",
    "    print(f\"after: {num_filtered_entries}\")\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(filtered_data, output_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"save as '{output_path}'\")\n",
    "\n",
    "file_path = '/home/qiyu/Dev/ziqing/wiki/enriched_KhanQ.json'\n",
    "\n",
    "output_path = 'filtered_enriched_KhanQ.json'\n",
    "\n",
    "remove_na_topics(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('filtered_enriched_KhanQ.json', 'r', encoding='utf-8') as file:\n",
    "    enriched_data = json.load(file)\n",
    "\n",
    "with open('wikified_KhanQ_text.json', 'r', encoding='utf-8') as file:\n",
    "    wikified_data = json.load(file)\n",
    "\n",
    "wikified_dict = {item[\"id\"]: item[\"annotations\"][\"annotation_data\"] for item in wikified_data}\n",
    "\n",
    "new_data = []\n",
    "\n",
    "for item in enriched_data:\n",
    "    item_id = item[\"id\"]\n",
    "    text = item[\"text\"]\n",
    "    topic = item[\"topic\"]\n",
    "\n",
    "    annotations = wikified_dict.get(item_id, [])\n",
    "    \n",
    "    highest_pagerank = None\n",
    "    highest_title = None\n",
    "    for annotation in annotations:\n",
    "        if annotation[\"title\"] != topic and (highest_pagerank is None or annotation[\"pageRank\"] > highest_pagerank):\n",
    "            highest_pagerank = annotation[\"pageRank\"]\n",
    "            highest_title = annotation[\"title\"]\n",
    "    \n",
    "\n",
    "    if highest_title:\n",
    "        new_data.append({\n",
    "            \"id\": item_id,\n",
    "            \"text\": text,\n",
    "            \"topic\": highest_title\n",
    "        })\n",
    "\n",
    "with open('othertopic_KhanQ.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(new_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"save as 'othertopic_KhanQ.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OneBit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
