{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import copy\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, early_stopping\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import time\n",
    "import os\n",
    "import re \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer\n",
    "    )\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import T5ForConditionalGeneration, T5Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/qiyu/Dev/ziqing/T5/doubled_newcombined_squad.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "train_df, temp_df = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df.to_csv('double_newcombined_squad_train.csv', index=False)\n",
    "\n",
    "val_df.to_csv('double_newcombined_squad_val.csv', index=False)\n",
    "\n",
    "test_df.to_csv('double_newcombined_squad_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_1 = pd.read_csv('/home/qiyu/Dev/ziqing/T5/train/newcombined_squad_train.csv')\n",
    "squad_val_1 = pd.read_csv('/home/qiyu/Dev/ziqing/T5/train/newcombined_squad_val.csv')\n",
    "squad_test_1 = pd.read_csv('/home/qiyu/Dev/ziqing/T5/train/newcombined_squad_test.csv')\n",
    "\n",
    "squad_train_1.head()\n",
    "squad_val_1.head()\n",
    "squad_test_1.head()\n",
    "\n",
    "train_1 = squad_train_1\n",
    "val_1 = squad_val_1\n",
    "test_1 = squad_test_1\n",
    "\n",
    "print('train_1_df:', train_1.shape,)\n",
    "print('val_1_df:', val_1.shape,)\n",
    "print('test_1_df:', test_1.shape,)\n",
    "\n",
    "train_1.head()\n",
    "\n",
    "SPECIAL_TOKENS = ['<sep>','<space>']\n",
    "MASKING_CHANCE = 0.3 \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "SOURCE_MAX_TOKEN_LEN = 200\n",
    "TARGET_MAX_TOKEN_LEN = 45\n",
    "\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "DF_TAKE_PERCENTAGE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class QGDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, tokenizer, file_path, max_len_input=512, max_len_output=128):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.data = pd.read_csv(file_path)\n",
    "#         self.max_len_input = max_len_input\n",
    "#         self.max_len_output = max_len_output\n",
    "#         self.context_column = 'text'\n",
    "#         self.topic = 'topic'\n",
    "#         self.question_column = 'question'\n",
    "#         self.inputs = []\n",
    "#         self.targets = []\n",
    "#         self._load_data()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.inputs)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         source_ids = self.inputs[index]['input_ids'].squeeze()\n",
    "#         target_ids = self.targets[index]['input_ids'].squeeze()\n",
    "#         source_mask = self.inputs[index]['attention_mask'].squeeze()\n",
    "#         target_mask = self.targets[index]['attention_mask'].squeeze()\n",
    "#         labels = copy.deepcopy(target_ids)\n",
    "#         labels[labels == 0] = -100\n",
    "#         return {'source_ids': source_ids, 'source_mask': source_mask, 'target_ids': target_ids, 'target_mask': target_mask, 'labels': labels}\n",
    "\n",
    "#     def _load_data(self):\n",
    "#         for idx in tqdm_notebook(range(len(self.data))):\n",
    "\n",
    "#             context, topic, target = self.data.loc[idx, self.context_column], self.data.loc[idx, self.topic], self.data.loc[idx, self.question_column]\n",
    "#             # if len(str(answer).split()) >= 8:\n",
    "#             #     input_text = '<longanswer> %s <context> %s ' % (answer, context)\n",
    "#             # else:\n",
    "#             #     input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "#             input_text = '<topic> %s <context> %s ' % (topic, context)\n",
    "#             target = str(target)\n",
    "\n",
    "#             tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "#                 [input_text],\n",
    "#                 max_length=self.max_len_input,\n",
    "#                 padding='max_length',\n",
    "#                 truncation=True,\n",
    "#                 return_tensors='pt'\n",
    "#             )\n",
    "\n",
    "#             tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "#                 [target],\n",
    "#                 max_length=self.max_len_output,\n",
    "#                 padding='max_length',\n",
    "#                 truncation=True,\n",
    "#                 return_tensors='pt'\n",
    "#             )\n",
    "\n",
    "#             self.inputs.append(tokenized_inputs)\n",
    "#             self.targets.append(tokenized_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, file_path, max_len_input=512, max_len_output=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_len_input = max_len_input\n",
    "        self.max_len_output = max_len_output\n",
    "        self.context_column = 'text'\n",
    "        self.topic = 'topic'\n",
    "        self.question_column = 'question'\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index]['input_ids'].squeeze()\n",
    "        target_ids = self.targets[index]['input_ids'].squeeze()\n",
    "        source_mask = self.inputs[index]['attention_mask'].squeeze()\n",
    "        target_mask = self.targets[index]['attention_mask'].squeeze()\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels[labels == 0] = -100\n",
    "        return {'source_ids': source_ids, 'source_mask': source_mask, 'target_ids': target_ids, 'target_mask': target_mask, 'labels': labels}\n",
    "\n",
    "    def _load_data(self):\n",
    "        for idx in tqdm_notebook(range(len(self.data))):\n",
    "\n",
    "            context, topic, target = self.data.loc[idx, self.context_column], self.data.loc[idx, self.topic], self.data.loc[idx, self.question_column]\n",
    "            # if len(str(answer).split()) >= 8:\n",
    "            #     input_text = '<longanswer> %s <context> %s ' % (answer, context)\n",
    "            # else:\n",
    "            #     input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "            input_text = f'{topic}<sep>{context}'\n",
    "            target = str(target)\n",
    "\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_text],\n",
    "                max_length=self.max_len_input,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target],\n",
    "                max_length=self.max_len_output,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# pl.seed_everything(99)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# print('Using device:', device)\n",
    "# print('Loading pre-trained model...')\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "# tokenizer.add_special_tokens(\n",
    "#         {'additional_special_tokens':  ['<sep>','<space>']}\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pl.seed_everything(99)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device:', device)\n",
    "print('Loading pre-trained model...')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing dataset...')\n",
    "train_dataset = QGDataset(tokenizer, '/home/qiyu/Dev/ziqing/T5/train/newcombined_squad_train.csv')\n",
    "validation_dataset = QGDataset(tokenizer, '/home/qiyu/Dev/ziqing/T5/train/newcombined_squad_val.csv')\n",
    "\n",
    "print('train_dataset: ', len(train_dataset))\n",
    "print('validation_dataset: ', len(validation_dataset))\n",
    "\n",
    "print ('Initializing model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<sep>','<space>']\n",
    "tokenizer.add_tokens(SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/home/qiyu/Dev/ziqing/T5/checkpoint_epoch_69.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer)-2)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded and ready for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.args = args\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels, # decoder_input_ids included in lm_labels\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            # decoder_input_ids=batch['target_ids'],\n",
    "            # decoder_attention_mask=batch['target_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        # logits = outputs.logits\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['source_ids'],\n",
    "            attention_mask=batch['source_mask'],\n",
    "            # decoder_input_ids=batch['target_ids'],\n",
    "            # decoder_attention_mask=batch['target_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        # logits = outputs.logits\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=64, num_workers=1)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(validation_dataset, batch_size=64, num_workers=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        # optimizer_grouped_parameters = [\n",
    "        #     {\n",
    "        #         \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        #         \"weight_decay\": self.args.weight_decay,\n",
    "        #     },\n",
    "        #     {\n",
    "        #         \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        #         \"weight_decay\": 0.0,\n",
    "        #     },\n",
    "        # ]\n",
    "        # return AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=args.eps)\n",
    "        return AdamW(self.parameters(), lr=0.001, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5FineTuner(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        \n",
    "        # gradient_clip_val=1.0,\n",
    "        # auto_lr_find=True,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '/home/qiyu/Dev/ziqing/T5/T5_Q8_newcombinedsquad_once'\n",
    "save_tokenizer_path = '/home/qiyu/Dev/ziqing/T5/T5_Q8_newcombinedsquad_once'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fine tuning...')\n",
    "trainer.fit(model)\n",
    "    # trainer.test()\n",
    "\n",
    "print('Saving model...')\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "if not os.path.exists(save_tokenizer_path):\n",
    "    os.makedirs(save_tokenizer_path)\n",
    "model.model.save_pretrained(save_model_path)\n",
    "tokenizer.save_pretrained(save_tokenizer_path)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print('Total time: %s hours' % (end_time / 60 / 60))\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train_2 = pd.read_csv('/home/qiyu/Dev/ziqing/T5/previous_squad/squad_train_2.csv')\n",
    "squad_val_2 = pd.read_csv('/home/qiyu/Dev/ziqing/T5/previous_squad/squad_val_2.csv')\n",
    "squad_test_2 = pd.read_csv('/home/qiyu/Dev/ziqing/T5/previous_squad/squad_test_2.csv')\n",
    "\n",
    "squad_train_2.head()\n",
    "squad_val_2.head()\n",
    "squad_test_2.head()\n",
    "\n",
    "train_2 = squad_train_2\n",
    "val_2 = squad_val_2\n",
    "test_2 = squad_test_2\n",
    "\n",
    "print('train_2_df:', train_2.shape,)\n",
    "print('val_2_df:', val_2.shape,)\n",
    "print('test_2_df:', test_2.shape,)\n",
    "\n",
    "train_2.head()\n",
    "\n",
    "SPECIAL_TOKENS = ['<sep>','<space>']\n",
    "MASKING_CHANCE = 0.3 #30% chance to replace the answer with '[MASK]'\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "SOURCE_MAX_TOKEN_LEN = 200\n",
    "TARGET_MAX_TOKEN_LEN = 45\n",
    "\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "DF_TAKE_PERCENTAGE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing dataset...')\n",
    "train_dataset = QGDataset(tokenizer, '/home/qiyu/Dev/ziqing/T5/previous_squad/squad_train_2.csv')\n",
    "validation_dataset = QGDataset(tokenizer, '/home/qiyu/Dev/ziqing/T5/previous_squad/squad_val_2.csv')\n",
    "\n",
    "print('train_dataset: ', len(train_dataset))\n",
    "print('validation_dataset: ', len(validation_dataset))\n",
    "\n",
    "print ('Initializing model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pl.seed_everything(99)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device:', device)\n",
    "print('Loading pre-trained model...')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/home/qiyu/Dev/ziqing/T5/T5stem_uniquesquad46_phase1\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/home/qiyu/Dev/ziqing/T5/T5stem_uniquesquad46_phase1\")\n",
    "tokenizer.add_special_tokens(\n",
    "        {'additional_special_tokens':  ['<sep>','<space>']}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5FineTuner(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        \n",
    "        # gradient_clip_val=1.0,\n",
    "        # auto_lr_find=True,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = '/home/qiyu/Dev/ziqing/T5/T5stem_uniquesquad46_phase2'\n",
    "save_tokenizer_path = '/home/qiyu/Dev/ziqing/T5/T5stem_uniquesquad46_phase2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fine tuning...')\n",
    "trainer.fit(model)\n",
    "    # trainer.test()\n",
    "\n",
    "print('Saving model...')\n",
    "if not os.path.exists(save_model_path):\n",
    "    os.makedirs(save_model_path)\n",
    "if not os.path.exists(save_tokenizer_path):\n",
    "    os.makedirs(save_tokenizer_path)\n",
    "model.model.save_pretrained(save_model_path)\n",
    "tokenizer.save_pretrained(save_tokenizer_path)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print('Total time: %s hours' % (end_time / 60 / 60))\n",
    "print('All done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OneBit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
