{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script takes in an unput csv with lecture id and text as an input and then uses wikifier to get the annotations for\n",
    "each topic\n",
    "\"\"\"\n",
    "import ujson as json\n",
    "from os import listdir\n",
    "from os.path import isfile, join, basename\n",
    "import requests\n",
    "\n",
    "import time\n",
    "from lib.api import get_wikifier_wikify_response\n",
    "from lib.text import segment_sentences\n",
    "from transcript_reader.utils import ENGLISH_FILE_REGEX\n",
    "\n",
    "_WIKIFIER_URL_PREFIX = \"https://en.wikipedia.org/wiki/\"\n",
    "\n",
    "_WIKIFIER_WIKIFY_URL = u\"http://www.wikifier.org/annotate-article\"\n",
    "\n",
    "CHUNK_SIZE = 15000\n",
    "BULK_SIZE = 50\n",
    "\n",
    "DF_IGNORE_VAL = 50\n",
    "WORDS_IGNORE_VAL = 50\n",
    "\n",
    "ACCURACY_FIELD = u'rho'\n",
    "TITLE_FIELD = u'title'\n",
    "COSINE_FIELD = u'cosine'\n",
    "PAGERANK_FIELD = u'pageRank'\n",
    "WIKI_DATA_ID_FIELD = u'wikiDataItemId'\n",
    "URL_FIELD = u'url'\n",
    "\n",
    "STATUS_FIELD = u'status'\n",
    "ANNOTATION_DATA_FIELD = u'annotation_data'\n",
    "\n",
    "SENTENCE_AGGREGATOR = \" \"\n",
    "LEN_SENTENCE_AGGR = len(SENTENCE_AGGREGATOR)\n",
    "\n",
    "SILENCE_INDICATORS = [\"~silence~\", \"~SILENCE~\", \"~SIL\", \"[SILENCE]\"]\n",
    "HESITATION_INDICATORS = [\"[hesitation]\", \"[HESITATION]\"]\n",
    "UNKNOWN_INDICATORS = [\"<unk>\", \"[UNKNOWN]\", \"[unknown]\"]\n",
    "\n",
    "SPECIAL_TOKENS = set(SILENCE_INDICATORS + HESITATION_INDICATORS + UNKNOWN_INDICATORS)\n",
    "\n",
    "FILEPATH_FIELD = \"filepath\"\n",
    "FILENAME_FIELD = \"filename\"\n",
    "SLUG_FIELD = \"slug\"\n",
    "TEXT_FIELD = \"text\"\n",
    "\n",
    "COLS = [FILEPATH_FIELD, SLUG_FIELD, TEXT_FIELD]\n",
    "\n",
    "ERROR_KEY = u'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# input_file = '/home/qiyu/Dev/ziqing/wiki/khan/tgt-train.txt'\n",
    "# output_file = 'tgt_train_ready.json'\n",
    "\n",
    "# with open(input_file, 'r', encoding='utf-8') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "\n",
    "# questions = []\n",
    "# for i, line in enumerate(lines):\n",
    "#     question_text = line.strip()\n",
    "#     questions.append({\"id\": i + 1, \"question\": question_text})\n",
    "\n",
    "\n",
    "# with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(questions, json_file, indent=4)\n",
    "\n",
    "# print(\"json completed, data saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_text(text, max_size):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        text:\n",
    "        min_size:\n",
    "        max_size:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    sentences = segment_sentences(text)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    temp_sents = []\n",
    "    temp_len = 0\n",
    "    for sentence in sentences:\n",
    "        len_sentence = len(sentence)\n",
    "        expected_len = temp_len + LEN_SENTENCE_AGGR + len_sentence  # estimate length cost\n",
    "        if expected_len > max_size:  # if it goes above threshold,\n",
    "            if len(temp_sents) > 0:\n",
    "                chunks.append(SENTENCE_AGGREGATOR.join(temp_sents))  # first load the preceding chunk\n",
    "                temp_sents = []\n",
    "                temp_len = 0\n",
    "\n",
    "        temp_sents.append(sentence)  # then aggregate the sentence to the temp chunk\n",
    "        temp_len += len_sentence\n",
    "\n",
    "    if len(temp_sents) > 0:\n",
    "        chunks.append(SENTENCE_AGGREGATOR.join(temp_sents))  # send the remainder chunk\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_clean_text(text):\n",
    "    for substr in SPECIAL_TOKENS:\n",
    "        text = text.replace(substr, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_wikififier_concepts(resp, prob=0.0, top_n=None):\n",
    "    annotations = list(sorted([{TITLE_FIELD: ann[TITLE_FIELD],\n",
    "                                URL_FIELD: ann[URL_FIELD],\n",
    "                                COSINE_FIELD: ann[COSINE_FIELD],\n",
    "                                PAGERANK_FIELD: ann[PAGERANK_FIELD],\n",
    "                                WIKI_DATA_ID_FIELD: ann.get(WIKI_DATA_ID_FIELD)}\n",
    "                               for ann in resp.get(\"annotations\", [])],\n",
    "                              key=lambda record: record[PAGERANK_FIELD], reverse=True))\n",
    "\n",
    "    if top_n is not None:\n",
    "        annotations = list(annotations)[:top_n]\n",
    "\n",
    "    return {\n",
    "        ANNOTATION_DATA_FIELD: annotations,\n",
    "        STATUS_FIELD: resp[STATUS_FIELD]\n",
    "    }\n",
    "\n",
    "def get_wikifier_wikify_response(text, api_key, df_ignore, words_ignore):\n",
    "    params = {\"text\": text, \"userKey\": api_key,\n",
    "            \"nTopDfValuesToIgnore\": df_ignore,\n",
    "            \"nWordsToIgnoreFromList\": words_ignore\n",
    "              }\n",
    "    r = requests.post(_WIKIFIER_WIKIFY_URL, params)\n",
    "    if r.status_code == 200:\n",
    "        resp = json.loads(r.content)\n",
    "        if ERROR_KEY in resp:\n",
    "            raise ValueError(\"error in response : {}\".format(resp[ERROR_KEY]))\n",
    "        return resp\n",
    "    else:\n",
    "        raise ValueError(\"http status code 200 expected, got status code {} instead\".format(r.status_code))\n",
    "\n",
    "\n",
    "def _wikify(text, key, df_ignore, words_ignore):\n",
    "    try:\n",
    "        resp = get_wikifier_wikify_response(text, key, df_ignore, words_ignore)\n",
    "        resp[STATUS_FIELD] = 'success'\n",
    "    except ValueError as e:\n",
    "        try:\n",
    "            STATUS_ = e.message\n",
    "        except:\n",
    "            STATUS_ = e.args[0]\n",
    "        return {\n",
    "            STATUS_FIELD: STATUS_\n",
    "        }\n",
    "    time.sleep(0.5)\n",
    "    return get_wikififier_concepts(resp, top_n=5)\n",
    "\n",
    "\n",
    "# def wikify_data(docs, wikifier_key):\n",
    "#     \"\"\" map partition function that parallaly calls the wikifier service\n",
    "\n",
    "#     Args:\n",
    "#         docs:\n",
    "#         wikifier_key:\n",
    "\n",
    "#     Yields:\n",
    "#         ({key: val}): dictionary with the annotations embedded to it\n",
    "#     \"\"\"\n",
    "#     enrichments = []\n",
    "#     for part in docs:\n",
    "#         annotations = _wikify(part[\"text\"], wikifier_key, DF_IGNORE_VAL, WORDS_IGNORE_VAL)\n",
    "#         part[\"annotations\"] = annotations\n",
    "#         enrichments.append(part)\n",
    "\n",
    "#         # print(\"video: {}:{}:{} is completed.\".format(part[\"slug\"], part[\"video_id\"], part[\"part\"]))\n",
    "\n",
    "#     return enrichments\n",
    "\n",
    "def wikify_data(docs, wikifier_key):\n",
    "    \"\"\"Map partition function that parallelly calls the wikifier service\n",
    "\n",
    "    Args:\n",
    "        docs: List of documents, each document is a dict with at least a \"text\" field.\n",
    "        wikifier_key: API key for the Wikifier service.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with annotations embedded.\n",
    "    \"\"\"\n",
    "    enrichments = []\n",
    "    for part in docs:\n",
    "        text_parts = partition_text(part[\"text\"],max_size=650)  # Define MAX_SIZE appropriately\n",
    "        part_annotations = []\n",
    "        for text_chunk in text_parts:\n",
    "            annotations = _wikify(text_chunk, wikifier_key, DF_IGNORE_VAL, WORDS_IGNORE_VAL)\n",
    "            part_annotations.append(annotations)\n",
    "        part[\"annotations\"] = part_annotations\n",
    "        enrichments.append(part)\n",
    "\n",
    "    return enrichments\n",
    "\n",
    "def _get_filename(filepath):\n",
    "    return basename(filepath)\n",
    "\n",
    "def get_wikifications_from_file(filepath, output_file_dir, wikifier_api_key):\n",
    "    with open(filepath) as infile:\n",
    "        lines = [json.loads(l) for l in infile.readlines() if l != \"\"]\n",
    "\n",
    "    if len(lines) == 0:\n",
    "        return {\"filepath\": filepath, \"status\": \"success: blank file\"}\n",
    "\n",
    "    annotations = list(wikify_data(lines, wikifier_api_key))\n",
    "\n",
    "    filename = _get_filename(filepath)\n",
    "\n",
    "    result_str = \"\\n\".join([json.dumps(anno) for anno in annotations])\n",
    "    with open(output_file_dir + filename, \"w\") as out:\n",
    "        out.write(result_str)\n",
    "\n",
    "    return {\"filepath\": filepath, \"status\": \"success\"}\n",
    "\n",
    "def main(input_filepath, output_filepath, wikifier_api_key):\n",
    "    with open(input_filepath, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    \n",
    "    enriched_data = wikify_data(data, wikifier_api_key)\n",
    "    \n",
    "    \n",
    "    with open(output_filepath, 'w') as outfile:\n",
    "        json.dump(enriched_data, outfile, indent=4)\n",
    "    \n",
    "    print(f\"Wikifier annotations saved to {output_filepath}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_filepath = '/home/qiyu/Dev/ziqing/wiki/ready_khan_computing.json'\n",
    "# output_filepath = '/home/qiyu/Dev/ziqing/wiki/wikified_khan_text_computing.json'\n",
    "# wikify_api_key = 'ffymhmwszzdvzrzxttemhghcofjnwn'\n",
    "# main(input_filepath, output_filepath, wikify_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_filepath = '/home/qiyu/Dev/ziqing/wiki/quarter_ready_squad_text.json'\n",
    "# output_filepath = '/home/qiyu/Dev/ziqing/wiki/wikified_quarter_squad_text.json'\n",
    "# wikify_api_key = 'ffymhmwszzdvzrzxttemhghcofjnwn'\n",
    "# main(input_filepath, output_filepath, wikify_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_filepath = '/home/qiyu/Dev/ziqing/wiki/two_quarter_ready_squad_text.json'\n",
    "# output_filepath = '/home/qiyu/Dev/ziqing/wiki/wikified_two_quarter_squad_text.json'\n",
    "# wikify_api_key = 'ffymhmwszzdvzrzxttemhghcofjnwn'\n",
    "# main(input_filepath, output_filepath, wikify_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_filepath = '/home/qiyu/Dev/ziqing/wiki/three_half_ready_squad_text.json'\n",
    "# output_filepath = '/home/qiyu/Dev/ziqing/wiki/wikified_three_half_squad_text.json'\n",
    "# wikify_api_key = 'ffymhmwszzdvzrzxttemhghcofjnwn'\n",
    "# main(input_filepath, output_filepath, wikify_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_filepath = '/home/qiyu/Dev/ziqing/wiki/1sthalf_khanexperiment1_text.json'\n",
    "# output_filepath = '/home/qiyu/Dev/ziqing/wiki/wikified_1sthalf_khanexperiment1_text.json'\n",
    "# wikify_api_key = 'ffymhmwszzdvzrzxttemhghcofjnwn'\n",
    "# main(input_filepath, output_filepath, wikify_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_filepath = '/home/qiyu/Dev/ziqing/wiki/2ndhalf_khanexperiment1_text.json'\n",
    "# output_filepath = '/home/qiyu/Dev/ziqing/wiki/wikified_2ndhalf_khanexperiment1_text.json'\n",
    "# wikify_api_key = 'ffymhmwszzdvzrzxttemhghcofjnwn'\n",
    "# main(input_filepath, output_filepath, wikify_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filepath = '/home/qiyu/Dev/ziqing/wiki/eval_squad_text.json'\n",
    "output_filepath = '/home/qiyu/Dev/ziqing/wiki/wikified_eval_squad_text.json'\n",
    "wikify_api_key = 'ffymhmwszzdvzrzxttemhghcofjnwn'\n",
    "main(input_filepath, output_filepath, wikify_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OneBit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
