{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('filtered_enriched_squad.csv')\n",
    "\n",
    "missing_data = df[df['question'].isnull() | df['topic'].isnull() | df['text'].isnull()]\n",
    "\n",
    "print(\"Rows with missing fields:\")\n",
    "print(missing_data)\n",
    "\n",
    "print(\"Number of rows with missing fields:\", missing_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "df = pd.read_csv('combined_squad.csv')\n",
    "\n",
    "def count_exceeding_tokens(topic, text, max_length=510):\n",
    "    combined_text = topic + \" \" + text  \n",
    "    token_count = len(tokenizer.encode(combined_text))\n",
    "    return token_count > max_length\n",
    "\n",
    "\n",
    "df['exceeds_limit'] = df.apply(lambda row: count_exceeding_tokens(row['topic'], row['text']), axis=1)\n",
    "\n",
    "exceeding_entries = df[df['exceeds_limit']]\n",
    "print(\"Entries exceeding 510 tokens:\")\n",
    "print(exceeding_entries[['question', 'topic', 'text']])\n",
    "exceeding_count = df['exceeds_limit'].sum()\n",
    "\n",
    "print(f\"Number of entries where 'topic' + 'text' exceed 510 tokens: {exceeding_count}\")\n",
    "\n",
    "df_filtered = df[~df['exceeds_limit']]\n",
    "\n",
    "df_filtered.drop(columns=['exceeds_limit'], inplace=True)\n",
    "\n",
    "df_filtered.to_csv('adjusted_combined_squad.csv', index=False)\n",
    "\n",
    "print(f\"Updated dataset saved. Number of entries now: {df_filtered.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "df = pd.read_csv('filtered_enriched_squad.csv')\n",
    "\n",
    "combined_df = pd.DataFrame(columns=['question', 'topic', 'text', 'context1', 'context2', 'question2', 'topic2'])\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "while len(combined_df) < 10001:\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        if len(combined_df) >= 10001:\n",
    "            break\n",
    "       \n",
    "        other_rows = df[(df.index != index) & (df['text'] != row['text'])]\n",
    "        if not other_rows.empty:\n",
    "         \n",
    "            other_index = np.random.choice(other_rows.index)\n",
    "            other_row = df.loc[other_index]\n",
    "\n",
    "            other_text = other_row['text']\n",
    "            other_question = other_row['question']\n",
    "            other_topic = other_row['topic']\n",
    "\n",
    "            combined_text = row['text'] + \" \" + other_text\n",
    "            token_count = len(tokenizer.encode(combined_text))\n",
    "\n",
    "            if token_count < 500:\n",
    "                new_row = pd.DataFrame({\n",
    "                    'question': [row['question']],\n",
    "                    'topic': [row['topic']],\n",
    "                    'text': [combined_text],\n",
    "                    'context1': [row['text']],\n",
    "                    'context2': [other_text],\n",
    "                    'question2': [other_question],\n",
    "                    'topic2': [other_topic]\n",
    "                })\n",
    "                combined_df = pd.concat([combined_df, new_row], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('2newcombined_squad.csv', index=False)\n",
    "\n",
    "print(\"Combined dataset created and saved as 'combined_squad.csv'.\")\n",
    "print(f\"Total entries in new dataset: {combined_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "combined_df = pd.read_csv('/home/qiyu/Dev/ziqing/T5/newcombined_squad.csv')\n",
    "\n",
    "duplicates = combined_df.duplicated(subset=['text'])\n",
    "\n",
    "print(\"Number of duplicate rows based on question, topic, and text:\", duplicates.shape[0])\n",
    "if duplicates.shape[0] > 0:\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicates found based on question, topic, and text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "df = pd.read_csv('/home/qiyu/Dev/ziqing/T5/2newcombined_squad.csv')\n",
    "\n",
    "def count_exceeding_tokens(topic, text, max_length=510):\n",
    "    combined_text = topic + \" \" + text  \n",
    "    token_count = len(tokenizer.encode(combined_text))\n",
    "    return token_count > max_length\n",
    "\n",
    "df['exceeds_limit'] = df.apply(lambda row: count_exceeding_tokens(row['topic2'], row['text']), axis=1)\n",
    "\n",
    "exceeding_entries = df[df['exceeds_limit']]\n",
    "print(\"Entries exceeding 510 tokens:\")\n",
    "print(exceeding_entries[['question', 'topic2', 'text']])\n",
    "exceeding_count = df['exceeds_limit'].sum()\n",
    "\n",
    "print(f\"Number of entries where 'topic2' + 'text' exceed 510 tokens: {exceeding_count}\")\n",
    "\n",
    "df_filtered = df[~df['exceeds_limit']]\n",
    "\n",
    "df_filtered.drop(columns=['exceeds_limit'], inplace=True)\n",
    "\n",
    "df_filtered.to_csv('adjusted_combined_squad.csv', index=False)\n",
    "\n",
    "print(f\"Updated dataset saved. Number of entries now: {df_filtered.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'newcombined_squad.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df_copy = df.copy()\n",
    "df_copy['text'] = df_copy['context2'] + \" \" + df_copy['context1']\n",
    "\n",
    "combined_df = pd.concat([df, df_copy], ignore_index=True)\n",
    "\n",
    "new_file_path = 'doubled_newcombined_squad.csv'\n",
    "combined_df.to_csv(new_file_path, index=False)\n",
    "\n",
    "print(f\"Doubled dataset saved as '{new_file_path}'.\")\n",
    "print(f\"Total entries in the new dataset: {combined_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small')\n",
    "\n",
    "df = pd.read_csv('/home/qiyu/Dev/ziqing/T5/filtered_enriched_KhanQ.csv')\n",
    "\n",
    "combined_df = pd.DataFrame(columns=['question', 'topic', 'text', 'question2', 'topic2', 'context1', 'context2'])\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "while len(combined_df) < 1000:\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        if len(combined_df) >= 1000:\n",
    "            break\n",
    "\n",
    "        other_indexes = df[df.index != index].index.tolist()\n",
    "\n",
    "        if len(other_indexes) > 0:\n",
    "     \n",
    "            other_index = np.random.choice(other_indexes)\n",
    "            other_row = df.loc[other_index]\n",
    "\n",
    "            new_text = row['text'] + \" \" + other_row['text']\n",
    "\n",
    "            token_count = len(tokenizer.encode(new_text))\n",
    "\n",
    "            if token_count < 500:\n",
    "               \n",
    "                new_row = {\n",
    "                    'question': row['question'],\n",
    "                    'topic': row['topic'],\n",
    "                    'text': new_text,\n",
    "                    'question2': other_row['question'],\n",
    "                    'topic2': other_row['topic'],\n",
    "                    'context1': row['text'],\n",
    "                    'context2': other_row['text']\n",
    "                }\n",
    "                new_row_df = pd.DataFrame(new_row, index=[0]) \n",
    "                combined_df = pd.concat([combined_df, new_row_df], ignore_index=True)  \n",
    "\n",
    "combined_df.to_csv('unready_combined_KhanQ.csv', index=False)\n",
    "\n",
    "print(\"Combined dataset created and saved as 'unready_combined_KhanQ.csv'.\")\n",
    "print(f\"Total entries in new dataset: {combined_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small')\n",
    "\n",
    "df = pd.read_csv('unready_combined_KhanQ.csv')\n",
    "\n",
    "def count_exceeding_tokens(topic, text, max_length=510):\n",
    "    combined_text = topic + \" \" + text \n",
    "    token_count = len(tokenizer.encode(combined_text))\n",
    "    return token_count > max_length\n",
    "\n",
    "df['exceeds_limit'] = df.apply(lambda row: count_exceeding_tokens(row['topic'], row['text']), axis=1)\n",
    "\n",
    "exceeding_entries = df[df['exceeds_limit']]\n",
    "print(\"Entries exceeding 510 tokens:\")\n",
    "print(exceeding_entries[['question', 'topic', 'text']])\n",
    "exceeding_count = df['exceeds_limit'].sum()\n",
    "\n",
    "print(f\"Number of entries where 'topic' + 'text' exceed 510 tokens: {exceeding_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "random_seed = 42\n",
    "data = pd.read_csv('unready_combined_KhanQ.csv')\n",
    "print(\"original:\", data.shape[0])\n",
    "print(data.head())\n",
    "sampled_data = data.sample(n=653, random_state=random_seed)\n",
    "print(\"after:\", sampled_data.shape[0])\n",
    "print(sampled_data.head())\n",
    "\n",
    "sampled_data.to_csv('combined_KhanQ.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "references_path = '/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_newcs_once/references_T5_combinedsquad_once.npy'\n",
    "predictions_path = '/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_newcs_once/predictions_T5_combinedsquad_once.npy'\n",
    "predictions_topic2_path = '/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_newcs_once/predictionstopic2_T5_combinedsquad_once.npy'\n",
    "\n",
    "references = np.load(references_path)\n",
    "predictions = np.load(predictions_path)\n",
    "predictions_topic2 = np.load(predictions_topic2_path)\n",
    "\n",
    "indices = np.random.choice(len(references), 30, replace=False)\n",
    "selected_references = references[indices]\n",
    "\n",
    "selected_predictions = predictions[indices]\n",
    "selected_predictions_topic2 = predictions_topic2[indices]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Index': indices,\n",
    "    'Label Question': selected_references,\n",
    "    'GQ1': selected_predictions,\n",
    "    'GQ2': selected_predictions_topic2\n",
    "})\n",
    "\n",
    "output_file = 'human_data.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data extracted and saved as '{output_file}'. Total entries: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/qiyu/Dev/ziqing/T5/human_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "np.random.seed(42) \n",
    "for idx in df.index:\n",
    "    if np.random.rand() > 0.5: \n",
    "        df.at[idx, 'GQ1'], df.at[idx, 'GQ2'] = df.at[idx, 'GQ2'], df.at[idx, 'GQ1']\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "lines = df.apply(lambda x: f\"Index: {x['Index']}\\nLabel Question: {x['Label Question']}\\nGQ1: {x['GQ1']}\\nGQ2: {x['GQ2']}\\n\", axis=1)\n",
    "\n",
    "output_file = 'shuffled_human_data.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "print(f\"Survey data prepared and saved as '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/home/qiyu/Dev/ziqing/T5/human_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['Swapped'] = 0 \n",
    "np.random.seed(42) \n",
    "for idx in df.index:\n",
    "    if np.random.rand() > 0.5: \n",
    "        df.at[idx, 'GQ1'], df.at[idx, 'GQ2'] = df.at[idx, 'GQ2'], df.at[idx, 'GQ1']\n",
    "        df.at[idx, 'Swapped'] = 1  \n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "output_file = 'shuffled_with_swap.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data with swap information saved as '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shuffled_data = pd.read_csv('shuffled_with_swap.csv')\n",
    "human_result = pd.read_csv('human_survey.csv')\n",
    "shuffled_data['Index'] = shuffled_data['Index'].astype(int)\n",
    "human_result['index'] = human_result['index'].astype(int)\n",
    "if not shuffled_data['Index'].is_unique:\n",
    "    print(\"shuffled_with_swap.csv has duplicate Index\")\n",
    "if not human_result['index'].is_unique:\n",
    "    print(\"human_survey.csv has duplicate Index\")\n",
    "data_merged = pd.merge(human_result, shuffled_data, left_on='index', right_on='Index', how='inner')\n",
    "\n",
    "if not data_merged['index'].is_unique:\n",
    "    print(\"has duplicate Index\")\n",
    "\n",
    "for student in ['student1', 'student2', 'student3', 'student4']:\n",
    "    data_merged[student] = ((data_merged[student] == 'GQ1') & (data_merged['Swapped'] == 0)) | \\\n",
    "                           ((data_merged[student] == 'GQ2') & (data_merged['Swapped'] == 1))\n",
    "\n",
    "data_merged[['student1', 'student2', 'student3', 'student4']] = data_merged[['student1', 'student2', 'student3', 'student4']].astype(int)\n",
    "\n",
    "result = data_merged[['index', 'student1', 'student2', 'student3', 'student4']]\n",
    "\n",
    "result.to_csv('human_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the human_result.csv file\n",
    "human_result = pd.read_csv('/home/qiyu/Dev/ziqing/T5/human_result.csv')\n",
    "\n",
    "# Create a list with each index duplicated immediately\n",
    "duplicated_indices = []\n",
    "for index in human_result['index']:\n",
    "    duplicated_indices.extend([index, index])  # Appends the same index twice consecutively\n",
    "\n",
    "# Create a new DataFrame using the list of duplicated indices\n",
    "new_df = pd.DataFrame({\n",
    "    'ID': duplicated_indices\n",
    "})\n",
    "# Add the 'Question Type' column with alternating 'rel' and 'non-rel' values\n",
    "new_df['Question Type'] = ['rel' if i % 2 == 0 else 'non-rel' for i in range(len(new_df))]\n",
    "\n",
    "# Add columns for 'human label', 'Relatedness Score', and 'BERTScore' with empty values for now\n",
    "new_df['Human Label'] = ''\n",
    "new_df['Relatedness Score'] = ''\n",
    "new_df['BERTScore'] = ''\n",
    "\n",
    "# Save to a new CSV file\n",
    "new_csv_filename = 'human_individual.csv'\n",
    "new_df.to_csv(new_csv_filename, index=False)\n",
    "\n",
    "# Print the path to the saved file\n",
    "print(f\"New CSV file created and saved to {new_csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the human_result.csv file\n",
    "human_result = pd.read_csv('/home/qiyu/Dev/ziqing/T5/human_result.csv')\n",
    "\n",
    "# Function to calculate the score based on the student responses\n",
    "def calculate_score(row):\n",
    "    if all(x == 1 for x in row):\n",
    "        return (1, 0)  # All 1s, rel=1, non-rel=0\n",
    "    else:\n",
    "        return (0.75, 0.25)  # At least one 0, rel=0.75, non-rel=0.25\n",
    "\n",
    "# Applying the function to each row and creating a new DataFrame for scores\n",
    "human_result['scores'] = human_result.apply(lambda row: calculate_score(row[['student1', 'student2', 'student3', 'student4']]), axis=1)\n",
    "scores_df = pd.DataFrame(human_result['scores'].tolist(), columns=['rel_score', 'non_rel_score'], index=human_result['index'])\n",
    "\n",
    "# Load the new_human_result.csv file\n",
    "new_human_result = pd.read_csv('/home/qiyu/Dev/ziqing/T5/human_individual.csv')\n",
    "\n",
    "# Function to update the scores based on 'id' and 'Question Type'\n",
    "def update_scores(row, scores_df):\n",
    "    if row['Question Type'] == 'rel':\n",
    "        return scores_df.loc[row['ID'], 'rel_score']\n",
    "    else:\n",
    "        return scores_df.loc[row['ID'], 'non_rel_score']\n",
    "\n",
    "# Apply the update function\n",
    "new_human_result['Human Label'] = new_human_result.apply(lambda row: update_scores(row, scores_df), axis=1)\n",
    "\n",
    "# Save the updated new_human_result.csv\n",
    "updated_csv_filename = 'human2_individual.csv'\n",
    "new_human_result.to_csv(updated_csv_filename, index=False)\n",
    "\n",
    "# Print the path to the saved file\n",
    "print(f\"Updated CSV file has been saved to {updated_csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "bert_gq1_lq_scores = pd.read_csv('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_newcs_once/BERT30_gq1_lq.csv')\n",
    "human_individual = pd.read_csv('/home/qiyu/Dev/ziqing/T5/human2_individual.csv')\n",
    "bert_gq2_lq_scores = pd.read_csv('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_newcs_once/BERT30_gq2_lq.csv')\n",
    "\n",
    "def update_bert_score1(row):\n",
    "    if row['Question Type'] == 'rel':\n",
    "        index = row['ID'] - 1 \n",
    "        if 0 <= index < len(bert_gq1_lq_scores):\n",
    "            return bert_gq1_lq_scores.iloc[index]['F1 Score']\n",
    "    return row['BERTScore']\n",
    "\n",
    "def update_bert_score2(row):\n",
    "    if row['Question Type'] == 'non-rel':\n",
    "        index = row['ID'] - 1 \n",
    "        if 0 <= index < len(bert_gq2_lq_scores):\n",
    "            return bert_gq2_lq_scores.iloc[index]['F1 Score']\n",
    "    return row['BERTScore']\n",
    "\n",
    "human_individual['BERTScore'] = human_individual.apply(update_bert_score1, axis=1)\n",
    "human_individual['BERTScore'] = human_individual.apply(update_bert_score2, axis=1)\n",
    "\n",
    "updated_csv_filename = 'human_individual.csv'\n",
    "human_individual.to_csv(updated_csv_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OneBit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
