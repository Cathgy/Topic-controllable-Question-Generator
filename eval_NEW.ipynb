{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_metric\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import copy\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from bert_score import score\n",
    "import textdescriptives as td\n",
    "import torch.quantization\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model_path = '/home/qiyu/Dev/ziqing/T5/T5_newcombinedsquad_once'\n",
    "trained_tokenizer_path = '/home/qiyu/Dev/ziqing/T5/T5_newcombinedsquad_once'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class QuestionGeneration:\n",
    "\n",
    "#     def __init__(self, model_dir=None):\n",
    "#         self.model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
    "#         self.tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer_path)\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.model = self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "\n",
    "#     def generate(self, topic: str, context: str):\n",
    "#         input_text = '<topic> %s <context> %s ' % (topic, context)\n",
    "#         encoding = self.tokenizer.encode_plus(\n",
    "#             input_text,\n",
    "#             return_tensors='pt'\n",
    "#         ).to(self.device)\n",
    "#         input_ids = encoding['input_ids']\n",
    "#         attention_mask = encoding['attention_mask']\n",
    "#         outputs = self.model.generate(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             num_beams = 10,\n",
    "#             num_return_sequences = 8\n",
    "#         )\n",
    "#         question_list = []\n",
    "#         for output in outputs:\n",
    "#             question = self.tokenizer.decode(\n",
    "#                 output,\n",
    "#                 skip_special_tokens=True,\n",
    "#                 clean_up_tokenization_spaces=True\n",
    "#             )\n",
    "#             question_list.append({'question': question, 'topic': topic, 'context': context})\n",
    "#         return question_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class QuestionGeneration:\n",
    "#     def __init__(self):\n",
    "#         self.model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
    "#         self.tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer_path)\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         self.model = self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "\n",
    "#     def process_context(self, topic, context):\n",
    "#         sentences = sent_tokenize(context)\n",
    "#         indices = [i for i, sentence in enumerate(sentences) if topic in sentence]\n",
    "        \n",
    "#         if len(indices) == 0:\n",
    "#          \n",
    "#             return context\n",
    "        \n",
    "#         \n",
    "#         start_index = max(0, indices[0] - 2)\n",
    "#         end_index = min(len(sentences), indices[-1] + 3) \n",
    "#         return \" \".join(sentences[start_index:end_index])\n",
    "\n",
    "#     def generate(self, topic: str, context: str):\n",
    "#         processed_context = self.process_context(topic, context)\n",
    "#         input_text = f'{topic}<sep>{context}'\n",
    "#         encoding = self.tokenizer.encode_plus(\n",
    "#             input_text,\n",
    "#             return_tensors='pt'\n",
    "#         ).to(self.device)\n",
    "#         input_ids = encoding['input_ids']\n",
    "#         attention_mask = encoding['attention_mask']\n",
    "#         outputs = self.model.generate(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             num_beams=10,\n",
    "#             num_return_sequences=8\n",
    "#         )\n",
    "#         question_list = []\n",
    "#         for output in outputs:\n",
    "#             question = self.tokenizer.decode(\n",
    "#                 output,\n",
    "#                 skip_special_tokens=True,\n",
    "#                 clean_up_tokenization_spaces=True\n",
    "#             )\n",
    "#             question_list.append({'question': question, 'topic': topic, 'context': processed_context})\n",
    "#         return question_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuestionGeneration:\n",
    "    def __init__(self):\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(trained_model_path, quantization_config=quantization_config)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer_path)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        #self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def process_context(self, topic, context):\n",
    "        sentences = sent_tokenize(context)\n",
    "        indices = [i for i, sentence in enumerate(sentences) if topic in sentence]\n",
    "        \n",
    "        if len(indices) == 0:\n",
    "            return context\n",
    "        \n",
    "        start_index = max(0, indices[0] - 2)\n",
    "        end_index = min(len(sentences), indices[-1] + 3)\n",
    "        return \" \".join(sentences[start_index:end_index])\n",
    "\n",
    "    def generate(self, topic: str, context: str):\n",
    "        processed_context = self.process_context(topic, context)\n",
    "        input_text = '<topic> {} <context> {} '.format(topic, processed_context)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            input_text,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            num_beams=10,\n",
    "            num_return_sequences=8\n",
    "        )\n",
    "        question_list = []\n",
    "        for output in outputs:\n",
    "            question = self.tokenizer.decode(\n",
    "                output,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True\n",
    "            )\n",
    "            question_list.append({'question': question, 'topic': topic, 'context': processed_context})\n",
    "        return question_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QGDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, file_path, max_len_input=512, max_len_output=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_len_input = max_len_input\n",
    "        self.max_len_output = max_len_output\n",
    "        self.context_column = 'text'\n",
    "        self.topic = 'topic'\n",
    "        self.question_column = 'question'\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index]['input_ids'].squeeze()\n",
    "        target_ids = self.targets[index]['input_ids'].squeeze()\n",
    "        source_mask = self.inputs[index]['attention_mask'].squeeze()\n",
    "        target_mask = self.targets[index]['attention_mask'].squeeze()\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels[labels == 0] = -100\n",
    "        return {'source_ids': source_ids, 'source_mask': source_mask, 'target_ids': target_ids, 'target_mask': target_mask, 'labels': labels}\n",
    "\n",
    "    def _load_data(self):\n",
    "        for idx in tqdm_notebook(range(len(self.data))):\n",
    "\n",
    "            context, topic, target = self.data.loc[idx, self.context_column], self.data.loc[idx, self.topic], self.data.loc[idx, self.question_column]\n",
    "            # if len(str(answer).split()) >= 8:\n",
    "            #     input_text = '<longanswer> %s <context> %s ' % (answer, context)\n",
    "            # else:\n",
    "            #     input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "            input_text = '<topic> %s <context> %s ' % (topic, context)\n",
    "            target = str(target)\n",
    "\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_text],\n",
    "                max_length=self.max_len_input,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target],\n",
    "                max_length=self.max_len_output,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QGDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, file_path, max_len_input=512, max_len_output=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.max_len_input = max_len_input\n",
    "        self.max_len_output = max_len_output\n",
    "        self.context_column = 'text'\n",
    "        self.topic = 'topic'\n",
    "        self.question_column = 'question'\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self._load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index]['input_ids'].squeeze()\n",
    "        target_ids = self.targets[index]['input_ids'].squeeze()\n",
    "        source_mask = self.inputs[index]['attention_mask'].squeeze()\n",
    "        target_mask = self.targets[index]['attention_mask'].squeeze()\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels[labels == 0] = -100\n",
    "        return {'source_ids': source_ids, 'source_mask': source_mask, 'target_ids': target_ids, 'target_mask': target_mask, 'labels': labels}\n",
    "\n",
    "    def _load_data(self):\n",
    "        for idx in tqdm_notebook(range(len(self.data))):\n",
    "\n",
    "            context, topic, target = self.data.loc[idx, self.context_column], self.data.loc[idx, self.topic], self.data.loc[idx, self.question_column]\n",
    "            # if len(str(answer).split()) >= 8:\n",
    "            #     input_text = '<longanswer> %s <context> %s ' % (answer, context)\n",
    "            # else:\n",
    "            #     input_text = '<answer> %s <context> %s ' % (answer, context)\n",
    "            input_text = f'{topic}<sep>{context}'\n",
    "            target = str(target)\n",
    "\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_text],\n",
    "                max_length=self.max_len_input,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target],\n",
    "                max_length=self.max_len_output,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = 'sentence-transformers/sentence-t5-base'\n",
    "\n",
    "# class SentenceEmbeddings:\n",
    "#     def __init__(self):\n",
    "#         self.embedder = SentenceTransformer(trained_model)\n",
    "#         self.nlp = spacy.load('en_core_web_sm')  \n",
    "\n",
    "#     def encode(self, text):\n",
    "#         return self.embedder.encode(text, convert_to_tensor=True)\n",
    "\n",
    "#     def split_into_sentence_groups(self, context):\n",
    "    \n",
    "#         doc = self.nlp(context)\n",
    "#         grouped_sentences = []\n",
    "#         temp_sentences = []\n",
    "        \n",
    "       \n",
    "#         for i, sentence in enumerate(doc.sents):\n",
    "#             temp_sentences.append(sentence.text)\n",
    "#             if (i + 1) % 3 == 0:\n",
    "#                 grouped_sentences.append(' '.join(temp_sentences))\n",
    "#                 temp_sentences = []\n",
    "        \n",
    "#         if temp_sentences:\n",
    "#             grouped_sentences.append(' '.join(temp_sentences))\n",
    "\n",
    "#         return grouped_sentences\n",
    "\n",
    "#     def get_most_similar(self, context: str, qa_list: list, text_weight=0.2, topic_weight=0.8):\n",
    "    \n",
    "#         paragraphs = self.split_into_sentence_groups(context)\n",
    "        \n",
    "#         paragraph_embeddings = {idx: self.encode(paragraph) for idx, paragraph in enumerate(paragraphs)}\n",
    "\n",
    "#         top1 = {'idx': None, 'score': float('-inf')}\n",
    "#         for i in range(len(qa_list)):\n",
    "#             topic_embeddings = self.encode(qa_list[i]['topic'])\n",
    "\n",
    "#             paragraph_scores = {idx: util.pytorch_cos_sim(topic_embeddings, par_emb) for idx, par_emb in paragraph_embeddings.items()}\n",
    "#             most_relevant_paragraph = max(paragraph_scores, key=paragraph_scores.get)\n",
    "\n",
    "#             question_embeddings = self.encode(qa_list[i]['question'])\n",
    "#             text_sim = util.pytorch_cos_sim(paragraph_embeddings[most_relevant_paragraph], question_embeddings)\n",
    "#             topic_sim = util.pytorch_cos_sim(topic_embeddings, question_embeddings)\n",
    "#             combined_score = (text_sim[0][0].item() * text_weight) + (topic_sim[0][0].item() * topic_weight)\n",
    "\n",
    "#             if combined_score > top1['score']:\n",
    "#                 top1['score'] = combined_score\n",
    "#                 top1['idx'] = i\n",
    "\n",
    "#         if top1['idx'] is not None:\n",
    "#             return qa_list[top1['idx']]\n",
    "#         else:\n",
    "#             return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_model = 'sentence-transformers/sentence-t5-base'\n",
    "\n",
    "class SentenceEmbeddings:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(trained_model)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.embedder.encode(text, convert_to_tensor=True)\n",
    "\n",
    "    def get_most_similar(self, context: str, qa_list: list, text_weight=0.2, topic_weight=0.8):\n",
    "        text_embeddings = self.encode(context)\n",
    "        top1 = {'idx': None, 'score': float('-inf')}\n",
    "        for i in range(len(qa_list)):\n",
    "            topic_embeddings = self.encode(qa_list[i]['topic'])\n",
    "            question_embeddings = self.encode(qa_list[i]['question'])\n",
    "            text_sim = util.pytorch_cos_sim(text_embeddings, question_embeddings)\n",
    "            topic_sim = util.pytorch_cos_sim(topic_embeddings, question_embeddings)\n",
    "            combined_score = (text_sim[0][0].item() * text_weight) + (topic_sim[0][0].item() * topic_weight)\n",
    "\n",
    "            if combined_score > top1['score']:\n",
    "                top1['score'] = combined_score\n",
    "                top1['idx'] = i\n",
    "\n",
    "        if top1['idx'] is not None:\n",
    "            return qa_list[top1['idx']]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KhanQ_dataset = load_dataset('csv', data_files = '/home/qiyu/Dev/ziqing/T5/combined_KhanQ.csv')\n",
    "QG = QuestionGeneration()\n",
    "SE = SentenceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating questions...')\n",
    "i = 0\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for d in tqdm_notebook(range(653)):\n",
    "    # if i > 3: break\n",
    "    # i += 1\n",
    "    topic = KhanQ_dataset['train']['topic2'][d]\n",
    "    question = KhanQ_dataset['train']['question2'][d]\n",
    "    context = KhanQ_dataset['train']['text'][d]\n",
    "    references.append(question)\n",
    "    qa_pair_list = QG.generate(topic, context)\n",
    "    generated_question = SE.get_most_similar(context, qa_pair_list)\n",
    "    predictions.append(generated_question['question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/references_T5_Q4_newcs_once.npy', references)\n",
    "np.save('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/predictions_T5_Q4_newcs_once.npy', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/referencestopic2_T5_Q4_newcs_once.npy', references)\n",
    "np.save('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/predictionstopic2_T5_Q4_newcs_once.npy', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = np.load('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/references_T5_Q4_newcs_once.npy').tolist()\n",
    "predictions = np.load('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/predictions_T5_Q4_newcs_once.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_topic2 = np.load('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/referencestopic2_T5_Q4_newcs_once.npy').tolist()\n",
    "predictions_topic2 = np.load('/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/predictionstopic2_T5_Q4_newcs_once.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Prediction VS Reference Example\n",
    "assert len(references) == len(predictions_topic2), \"The number of references and predictions must be the same.\"\n",
    "\n",
    "for i, (ref, pred) in enumerate(zip(references, predictions_topic2)):\n",
    "    print(f\"Comparison {i+1}:\")\n",
    "    print(f\"Reference: {ref}\")\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(predictions, references, weights):\n",
    "    bs = []\n",
    "    smooth = SmoothingFunction()\n",
    "    for i in range(len(predictions)):\n",
    "        bleu_score = sentence_bleu([references[i]], predictions[i], weights= weights, smoothing_function=smooth.method2)\n",
    "        bs.append(bleu_score)\n",
    "    return bs\n",
    "\n",
    "n_gram = 0\n",
    "weights = []\n",
    "if n_gram == 1:\n",
    "    weights = [1, 0, 0, 0]\n",
    "elif n_gram == 2:\n",
    "    weights = [0, 1, 0, 0]\n",
    "elif n_gram == 3:\n",
    "    weights = [0, 0, 1, 0]\n",
    "elif n_gram == 4:\n",
    "    weights = [0, 0, 0, 1]\n",
    "elif n_gram == 0:  # Represent mix\n",
    "    weights = [0.25, 0.25, 0.25, 0.25]\n",
    "    \n",
    "\n",
    "bs1 = np.array(compute_bleu(predictions, references, weights=[1,0,0,0]))\n",
    "bs2 = np.array(compute_bleu(predictions, references, weights=[0,1,0,0]))\n",
    "bs3 = np.array(compute_bleu(predictions, references, weights=[0,0,1,0]))\n",
    "bs4 = np.array(compute_bleu(predictions, references, weights=[0,0,0,1]))\n",
    "bsmix = np.array(compute_bleu(predictions, references, weights=[0.25,0.25,0.25,0.25]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "BLEU Score Comparison:\n",
    "-------------------------------------------------\n",
    "1-gram BLEU Score:\n",
    "    Prediction = {bs1.mean()}\n",
    "-------------------------------------------------\n",
    "2-gram BLEU Score:\n",
    "    Prediction = {bs2.mean()}\n",
    "-------------------------------------------------\n",
    "3-gram BLEU Score:\n",
    "    Prediction = {bs3.mean()}\n",
    "-------------------------------------------------\n",
    "4-gram BLEU Score:\n",
    "    Prediction = {bs4.mean()}\n",
    "-------------------------------------------------\n",
    "Mixed BLEU Score:\n",
    "    Prediction = {bsmix.mean()}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/bleu_T5_Q4_newcs_once.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "# Displaying the file path for the user to download\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load_metric('rouge')\n",
    "bleu = load_metric('bleu')\n",
    "meteor = load_metric('meteor')\n",
    "\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "print(rouge_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "ROUGE Score Comparison:\n",
    "-------------------------------------------------\n",
    "**ROUGE-1 Scores:**\n",
    "    Precision - Low: {rouge_score['rouge1'].low.precision:.2%}, Mid: {rouge_score['rouge1'].mid.precision:.2%}, High: {rouge_score['rouge1'].high.precision:.2%}    \n",
    "    Recall - Low: {rouge_score['rouge1'].low.recall:.2%}, Mid: {rouge_score['rouge1'].mid.recall:.2%}, High: {rouge_score['rouge1'].high.recall:.2%}    \n",
    "    F-measure - Low: {rouge_score['rouge1'].low.fmeasure:.2%}, Mid: {rouge_score['rouge1'].mid.fmeasure:.2%}, High: {rouge_score['rouge1'].high.fmeasure:.2%}\n",
    "    -------------------------------------------------\n",
    "**ROUGE-2 Scores:**\n",
    "    Precision - Low: {rouge_score['rouge2'].low.precision:.2%}, Mid: {rouge_score['rouge2'].mid.precision:.2%}, High: {rouge_score['rouge2'].high.precision:.2%}\n",
    "    Recall - Low: {rouge_score['rouge2'].low.recall:.2%}, Mid: {rouge_score['rouge2'].mid.recall:.2%}, High: {rouge_score['rouge2'].high.recall:.2%}\n",
    "    F-measure - Low: {rouge_score['rouge2'].low.fmeasure:.2%}, Mid: {rouge_score['rouge2'].mid.fmeasure:.2%}, High: {rouge_score['rouge2'].high.fmeasure:.2%}\n",
    "-------------------------------------------------\n",
    "**ROUGE-L Scores:**\n",
    "    Precision - Low: {rouge_score['rougeL'].low.precision:.2%}, Mid: {rouge_score['rougeL'].mid.precision:.2%}, High: {rouge_score['rougeL'].high.precision:.2%}\n",
    "    Recall - Low: {rouge_score['rougeL'].low.recall:.2%}, Mid: {rouge_score['rougeL'].mid.recall:.2%}, High: {rouge_score['rougeL'].high.recall:.2%}\n",
    "    F-measure - Low: {rouge_score['rougeL'].low.fmeasure:.2%}, Mid: {rouge_score['rougeL'].mid.fmeasure:.2%}, High: {rouge_score['rougeL'].high.fmeasure:.2%}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "# Print the results\n",
    "print(result_content)\n",
    "\n",
    "# Save results to a variable, or alternatively write to a file\n",
    "rouge_score_T5squad46 = result_content\n",
    "# with open(\"rouge_score_T5squad46.txt\", \"w\") as file:\n",
    "#     file.write(result_content)\n",
    "\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/rouge_T5_Q4_newcs_once.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "# Displaying the file path for the user to download\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "print(meteor_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "METEOR Score Comparison:\n",
    "-------------------------------------------------\n",
    "METEOR Score: {meteor_results}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Print the results to check output\n",
    "print(result_content)\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/meteor_T5_Q4_newcs_once.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(generated, labels):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for gen, lab in zip(generated, labels):\n",
    "        gen_tokens = set(word_tokenize(gen.lower()))\n",
    "        lab_tokens = set(word_tokenize(lab.lower()))\n",
    "        \n",
    "        common_tokens = gen_tokens.intersection(lab_tokens)\n",
    "        if len(gen_tokens) == 0 or len(lab_tokens) == 0:\n",
    "            continue\n",
    "        \n",
    "        precision = len(common_tokens) / len(gen_tokens)\n",
    "        recall = len(common_tokens) / len(lab_tokens)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    if precisions and recalls:\n",
    "        mean_precision = sum(precisions) / len(precisions)\n",
    "        mean_recall = sum(recalls) / len(recalls)\n",
    "        if mean_precision + mean_recall != 0:\n",
    "            f1_score = 2 * (mean_precision * mean_recall) / (mean_precision + mean_recall)\n",
    "        else:\n",
    "            f1_score = 0\n",
    "        return mean_precision, mean_recall, f1_score\n",
    "    else:\n",
    "        return 0, 0, 0  # No valid data to calculate scores\n",
    "\n",
    "precision, recall, f1 = calculate_f1_score(predictions, references)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "F1 Score Comparison:\n",
    "-------------------------------------------------\n",
    "F1 Score: {f1}\n",
    "\n",
    "Precision: {precision}\n",
    "\n",
    "Recall: {recall}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Print the results to check output\n",
    "print(result_content)\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/f1_T5_Q4_newcs_once.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textdescriptives/information_theory\")\n",
    "\n",
    "\n",
    "prediction_texts = [str(pred) for pred in predictions]\n",
    "reference_texts = [str(ref) for ref in references]\n",
    "\n",
    "\n",
    "def calculate_perplexity(texts):\n",
    "    docs = list(nlp.pipe(texts))\n",
    "    perplexities = [doc._.perplexity for doc in docs]\n",
    "    return perplexities\n",
    "\n",
    "prediction_perplexities = calculate_perplexity(prediction_texts)\n",
    "reference_perplexities = calculate_perplexity(reference_texts)\n",
    "\n",
    "# Calculate average perplexities\n",
    "average_prediction_perplexity = np.mean(prediction_perplexities)\n",
    "average_reference_perplexity = np.mean(reference_perplexities)\n",
    "\n",
    "# Step 4: Print the results\n",
    "print(f\"Average Prediction Perplexity: {average_prediction_perplexity}\")\n",
    "print(f\"Average Reference Perplexity: {average_reference_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "Perplexity(spaCy):\n",
    "-------------------------------------------------\n",
    "Perplexiyu: {average_prediction_perplexity.item()}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Print the results to check output\n",
    "print(result_content)\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/PPL_T5_Q4_newcs_once.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "# Displaying the file path for the user to download\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_sentences(sentence_list):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentence_list:\n",
    "        # Tokenize the sentence\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Remove stopwords and question marks\n",
    "        filtered_sentence = ' '.join([word for word in words if word.lower() not in stop_words and word != '?'])\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "    return filtered_sentences\n",
    "\n",
    "clean_references = clean_sentences(references)\n",
    "clean_predictions = clean_sentences(predictions)\n",
    "clean_references_topic2 = clean_sentences(references_topic2)\n",
    "clean_predictions_topic2 = clean_sentences(predictions_topic2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########Example\n",
    "# Optional: Check a few entries to ensure cleaning is done\n",
    "print(\"Original vs Cleaned:\")\n",
    "for original, cleaned in zip(references[:5], clean_references[:5]):\n",
    "    print(\"Original:\", original)\n",
    "    print(\"Cleaned:\", cleaned)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert len(clean_references) == len(clean_predictions), \"The number of references and predictions must be the same.\"\n",
    "\n",
    "\n",
    "P, R, F1 = score(clean_predictions, clean_references, lang=\"en\", verbose=True)\n",
    "\n",
    "\n",
    "print(f\"Average Precision: {P.mean().item()}\")\n",
    "print(f\"Average Recall: {R.mean().item()}\")\n",
    "print(f\"Average F1 Score: {F1.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Individual Score Example\n",
    "for idx, (p, r, f1) in enumerate(zip(P, R, F1)):\n",
    "    print(f\"Comparison {idx+1}:\")\n",
    "    print(f\"Reference: {references[idx]}\")\n",
    "    print(f\"Prediction: {predictions[idx]}\")\n",
    "    print(f\"Precision: {p:.4f}, Recall: {r:.4f}, F1 Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store results\n",
    "results_df = pd.DataFrame({\n",
    "    'Reference': references,\n",
    "    'Prediction': predictions,\n",
    "    'Precision': P.tolist(),\n",
    "    'Recall': R.tolist(),\n",
    "    'F1 Score': F1.tolist()\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_filename = 'BERT30_gq1_lq.csv'\n",
    "results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "# Print the path to the saved file\n",
    "print(f\"Results have been saved to {results_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "BERT Score Comparison(cleaned):\n",
    "-------------------------------------------------\n",
    "BERT Precision: {P.mean().item()}\n",
    "\n",
    "BERT Recall: {R.mean().item()}\n",
    "\n",
    "BERT F1: {F1.mean().item()}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Print the results to check output\n",
    "print(result_content)\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/BERT_gq1_lq.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert len(clean_references) == len(clean_predictions_topic2), \"The number of references and predictions must be the same.\"\n",
    "\n",
    "\n",
    "P, R, F1 = score(clean_predictions_topic2, clean_references, lang=\"en\", verbose=True)\n",
    "\n",
    "\n",
    "print(f\"Average Precision: {P.mean().item()}\")\n",
    "print(f\"Average Recall: {R.mean().item()}\")\n",
    "print(f\"Average F1 Score: {F1.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Individual Score Example\n",
    "for idx, (p, r, f1) in enumerate(zip(P, R, F1)):\n",
    "    print(f\"Comparison {idx+1}:\")\n",
    "    print(f\"Reference: {clean_references[idx]}\")\n",
    "    print(f\"Prediction: {clean_predictions_topic2[idx]}\")\n",
    "    print(f\"Precision: {p:.4f}, Recall: {r:.4f}, F1 Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Reference': references,\n",
    "    'Prediction': predictions_topic2,\n",
    "    'Precision': P.tolist(),\n",
    "    'Recall': R.tolist(),\n",
    "    'F1 Score': F1.tolist()\n",
    "})\n",
    "\n",
    "\n",
    "results_filename = 'BERT30_gq2_lq.csv'\n",
    "results_df.to_csv(results_filename, index=False)\n",
    "\n",
    "\n",
    "print(f\"Results have been saved to {results_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "BERT Score Comparison(cleaned):\n",
    "-------------------------------------------------\n",
    "BERT Precision: {P.mean().item()}\n",
    "\n",
    "BERT Recall: {R.mean().item()}\n",
    "\n",
    "BERT F1: {F1.mean().item()}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Print the results to check output\n",
    "print(result_content)\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/BERT_gq2_lq.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('/home/qiyu/Dev/ziqing/T5/combined_KhanQ.csv')\n",
    "context1 = df['context1'].tolist()\n",
    "context2 = df['context2'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert len(context1) == len(clean_predictions_topic2), \"The number of references and predictions must be the same.\"\n",
    "\n",
    "\n",
    "P, R, F1 = score(clean_predictions_topic2, context1, lang=\"en\", verbose=True)\n",
    "\n",
    "\n",
    "print(f\"Average Precision: {P.mean().item()}\")\n",
    "print(f\"Average Recall: {R.mean().item()}\")\n",
    "print(f\"Average F1 Score: {F1.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "BERT Score Comparison(cleaned):\n",
    "-------------------------------------------------\n",
    "BERT Precision: {P.mean().item()}\n",
    "\n",
    "BERT Recall: {R.mean().item()}\n",
    "\n",
    "BERT F1: {F1.mean().item()}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Print the results to check output\n",
    "print(result_content)\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/BERT_gq2_ctx1.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert len(context2) == len(clean_predictions_topic2), \"The number of references and predictions must be the same.\"\n",
    "\n",
    "\n",
    "P, R, F1 = score(clean_predictions_topic2, context2, lang=\"en\", verbose=True)\n",
    "\n",
    "\n",
    "print(f\"Average Precision: {P.mean().item()}\")\n",
    "print(f\"Average Recall: {R.mean().item()}\")\n",
    "print(f\"Average F1 Score: {F1.mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_content = f\"\"\"\n",
    "BERT Score Comparison(cleaned):\n",
    "-------------------------------------------------\n",
    "BERT Precision: {P.mean().item()}\n",
    "\n",
    "BERT Recall: {R.mean().item()}\n",
    "\n",
    "BERT F1: {F1.mean().item()}\n",
    "-------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "# Print the results to check output\n",
    "print(result_content)\n",
    "\n",
    "# Write the content to a txt file\n",
    "file_path = \"/home/qiyu/Dev/ziqing/T5/train/eval_newcsQGSE/T5_Q4/BERT_qg2_ctx2.txt\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(result_content)\n",
    "\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "scores_topicqgedu = [0.536, 0.328, 0.221, 0.177, 0.321,0.22,1.345,0.2159]\n",
    "scores_topicqg = [0.551, 0.343, 0.236, 0.191, 0.33,0.233,1.323,0.2295]\n",
    "\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(scores_topicqgedu, scores_topicqg)\n",
    "\n",
    "print(f't : {t_stat}, p : {p_value}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OneBit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
